{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch入门与实战第六课 \n",
    "\n",
    "褚则伟 zeweichu@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目录\n",
    "- 图片风格迁移 \n",
    "- 用GAN生成MNIST\n",
    "- 用DCGAN生成更复杂的图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片风格迁移 Neural Style Transfer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A Neural Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf)\n",
    "本文介绍了Neural Style Transfor模型\n",
    "\n",
    "\n",
    "[Demystifying Neural Style Transfer](https://arxiv.org/pdf/1701.01036.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'png/content.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7166c897ce6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m ]) # 来自ImageNet的mean和variance\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"png/content.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mstype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"png/style.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-7166c897ce6e>\u001b[0m in \u001b[0;36mload_image\u001b[1;34m(image_path, transform, max_size, shape)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_size\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2842\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2843\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'png/content.png'"
     ]
    }
   ],
   "source": [
    "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
    "    image = Image.open(image_path)\n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size= np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "         \n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "        \n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "        \n",
    "    return image.to(device)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "]) # 来自ImageNet的mean和variance\n",
    "\n",
    "content = load_image(\"png/content.png\", transform, max_size=400)\n",
    "stype = load_image(\"png/style.png\", transform, shape=[content.size(2), content.size(3)])\n",
    "\n",
    "# content = load_image(\"png/content.png\", transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ]), max_size=400)\n",
    "# style = load_image(\"png/style.png\", transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ]), shape=[content.size(2), content.size(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stype.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "imshow(style[0], title='Image')\n",
    "# content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28']\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "target = content.clone().requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([target], lr=0.003, betas=[0.5, 0.999])\n",
    "vgg = VGGNet().to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = vgg(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = 2000\n",
    "style_weight = 100.\n",
    "for step in range(total_step):\n",
    "    target_features = vgg(target)\n",
    "    content_features = vgg(content)\n",
    "    style_features = vgg(style)\n",
    "    \n",
    "    style_loss = 0\n",
    "    content_loss = 0\n",
    "    for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "        content_loss += torch.mean((f1-f2)**2)\n",
    "        _, c, h, w = f1.size()\n",
    "        f1 = f1.view(c, h*w)\n",
    "        f3 = f3.view(c, h*w)\n",
    "        \n",
    "        # 计算gram matrix\n",
    "        f1 = torch.mm(f1, f1.t())\n",
    "        f3 = torch.mm(f3, f3.t())\n",
    "        style_loss += torch.mean((f1-f3)**2)/(c*h*w)\n",
    "        \n",
    "    loss = content_loss + style_weight * style_loss\n",
    "    \n",
    "    # 更新target\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(\"Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}\"\n",
    "             .format(step, total_step, content_loss.item(), style_loss.item()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "img = target.clone().squeeze()\n",
    "img = denorm(img).clamp_(0, 1)\n",
    "plt.figure()\n",
    "imshow(img, title='Target Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                        std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "mnist_data = torchvision.datasets.MNIST(\"./mnist_data\", train=True, download=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=mnist_data,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 784\n",
    "\n",
    "hidden_size = 256\n",
    "# discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "latent_size = 64\n",
    "# Generator\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "D = D.to(device)\n",
    "G = G.to(device)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()\n",
    "\n",
    "total_step = len(dataloader)\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(dataloader):\n",
    "        batch_size = images.size(0)\n",
    "        images = images.reshape(batch_size, image_size).to(device)\n",
    "        \n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        outputs = D(images)\n",
    "        d_loss_real = loss_fn(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # 开始生成fake images\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images.detach())\n",
    "        d_loss_fake = loss_fn(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # 开始优化discriminator\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        reset_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # 开始优化generator\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        g_loss = loss_fn(outputs, real_labels)\n",
    "        \n",
    "        reset_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}\"\n",
    "                 .format(epoch, num_epochs, i, total_step, d_loss.item(), g_loss.item(), real_score.mean().item(), fake_score.mean().item()))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fake images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = torch.randn(1, latent_size).to(device)\n",
    "fake_images = G(z).view(28, 28).data.cpu().numpy()\n",
    "plt.imshow(fake_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "真实图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0].view(28,28).data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1511.06434.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[图片下载地址](https://drive.google.com/drive/folders/0B7EVK8r0v71pbWNEUjJKdDQ3dGc)\n",
    "https://drive.google.com/drive/folders/0B7EVK8r0v71pbWNEUjJKdDQ3dGc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls celeba/img_align_celeba/img_align_celeba_png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=64\n",
    "batch_size=128\n",
    "dataroot=\"celeba/img_align_celeba\"\n",
    "num_workers = 2\n",
    "dataset = torchvision.datasets.ImageFolder(root=dataroot, transform=transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch=next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis=(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(), (1,2,0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把模型的所有参数都初始化城mean=0, std=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dcgan](images/dcgan_generator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100 # latent vector的大小\n",
    "ngf = 64 # generator feature map size\n",
    "ndf = 64 # discriminator feature map size\n",
    "nc = 3 # color channels\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            # torch.nn.ConvTranspose2d(in_channels, out_channels, \n",
    "            # kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can instantiate the generator and apply the weights_init function. Check out the printed model to see how the generator object is structured.\n",
    "\n",
    "# Create the generator\n",
    "netG = Generator().to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, as with the generator, we can create the discriminator, apply the weights_init function, and print the model’s structure.\n",
    "\n",
    "# Create the Discriminator\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "d_optimizer = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "g_optimizer = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # 训练discriminator, maximize log(D(x)) + log(1-D(G(z)))\n",
    "        \n",
    "        # 首先训练真实图片\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_images = data[0].to(device)\n",
    "        b_size = real_images.size(0)\n",
    "        label = torch.ones(b_size).to(device)\n",
    "        output = netD(real_images).view(-1)\n",
    "        \n",
    "        \n",
    "        real_loss = loss_fn(output, label)\n",
    "        real_loss.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        \n",
    "        # 然后训练生成的假图片\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake_images = netG(noise)\n",
    "        label.fill_(0)\n",
    "        output = netD(fake_images.detach()).view(-1)\n",
    "        fake_loss = loss_fn(output, label)\n",
    "        fake_loss.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        loss_D = real_loss + fake_loss\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # 训练Generator \n",
    "        netG.zero_grad()\n",
    "        label.fill_(1)\n",
    "        output = netD(fake_images).view(-1)\n",
    "        loss_G = loss_fn(output, label)\n",
    "        loss_G.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"[{}/{}] [{}/{}] Loss_D: {:.4f} Loss_G {:.4f} D(x): {:.4f} D(G(z)): {:.4f}/{:.4f}\"\n",
    "                 .format(epoch, num_epochs, i, len(dataloader), loss_D.item(), loss_G.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        G_losses.append(loss_G.item())\n",
    "        D_losses.append(loss_D.item())\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fake = netG(fixed_noise).detach().cpu()\n",
    "# fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis=(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis=(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(fake, padding=2, normalize=True), (1,2,0)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
